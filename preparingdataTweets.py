# -*- coding: utf-8 -*-
"""PreparingDataTweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
   https://colab.research.google.com/drive/1BQkrFBEXD4yohvOdXJrkei-agXpImqkd
"""

import pickle
import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
import sys

#matplotlib inline

import sys

from google.colab import drive
drive.mount('/content/drive')

experiment_folder="/content/drive/My Drive/PSU_sentiment_analysis_session"
sys.path.append(experiment_folder)

!pip install farasapy

from google.colab import files

!cp /content/drive/MyDrive/Modules/aranorm.py /content/

import aranorm

import tweepy
from tqdm import tqdm
import pandas as pd
import pickle


def lookup_tweets(tweet_IDs, api):
    full_tweets = []
    tweet_count = len(tweet_IDs)
    try:
        for i in tqdm(range(int(tweet_count / 100) + 1)):
             #Catch the last group if it is less than 100 tweets
             end_loc = min((i + 1) * 100, tweet_count)
             full_tweets.extend(
                 api.statuses_lookup(id_=tweet_IDs[i * 100:end_loc])
             )
            
    except tweepy.TweepError as e:
         print(e)
         print('Something went wrong, quitting...')
    
    return full_tweets


consumer_key = 'pxmVOs7kh1tSaJfwAo349RMUv'
consumer_secret = 'oNiILZlT14iBmIN5WjEOIpAvaA9T34JhlmjTiwPM9yC2Iifk3g'
access_token = '1505641464113647632-X3jpikvgNRYTfnGmkhNHMFBSHj7Lfj'
access_token_secret = '2YO9BJRAaffKgOs9VtJSEEUnfmUE9yaMowS09r2gsIeev'

print('Connecting to Twitter API...')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
print('Connected!!')

print('Loading Tweets IDs file...')
filename = 'official_ASAD.csv'

data = pd.read_csv(filename)
tweets_ids = list(data['Tweet_id'])

print('Fetching data from API....')
results = lookup_tweets(tweets_ids, api)

tweets_jsons = []
for tweet in results:
     tweets_jsons.append(tweet._json)

with open('train_tweets_json.pkl', 'wb') as handle:
    pickle.dump(tweets_jsons, handle, protocol=pickle.HIGHEST_PROTOCOL)

#pickle.dump(tweets_jsons, open('/train_tweets_json.pkl', 'wb'))

print('Loading Tweets IDs file...')
filename = 'official_ASAD.csv'

data = pd.read_csv(filename)
tweets_ids = list(data['Tweet_id'])

print(tweets_ids[0])

with open('train_tweets_json.pkl', 'rb') as handle:
    tweets = pickle.load(handle)

len(tweets)

tweets[0]

tweets_by_id = {}
for tweet in tweets:
    tweets_by_id[tweet['id']] = tweet

def prepare_text(x):
    tweet_id = x['Tweet_id']
    if tweet_id in tweets_by_id:
        tweet_text = tweets_by_id[tweet_id]['text']
        x['text'] = tweet_text
        x['normalized_text'] = aranorm.normalize_arabic_text(x['text'])
    return x

data = pd.read_csv('official_ASAD.csv')
data.head()

data['text'] = "UNKOWN_TEXT!"
data.head()

data = data.apply(prepare_text, axis=1)

data.head()

data = data[data['text'] != 'UNKOWN_TEXT!'].reset_index(drop=True) # remove tweets that have no text
data.head()

data.shape

data = data.drop_duplicates('normalized_text').reset_index(drop=True) # remove duplicate data
len(data)

len(data[data['sentiment'] == 'Positive']), \
len(data[data['sentiment'] == 'Neutral']), \
len(data[data['sentiment'] == 'Negative'])

def show_pie_chart(data_dist):
    #define Seaborn color palette to use
    red = [(0.8901960784313725, 0.10196078431372549, 0.10980392156862745)]
    orange = [(1.0, 0.4980392156862745, 0.0)]
    green = [(0.2, 0.6274509803921569, 0.17254901960784313)]
    labels = ['Positive', 'Neutral', 'Negative']
    #create pie chart
    plt.pie(data_dist, labels = labels, colors=green+orange+red, autopct='%.0f%%')
    plt.show()

data_dist = [
                len(data[data['sentiment'] == 'Positive']),
                len(data[data['sentiment'] == 'Neutral']),
                len(data[data['sentiment'] == 'Negative'])
            ]
show_pie_chart(data_dist)

"""One way to balance the number of examples in all classes is to find the class that has the minimum number of examples then randomly sample the same amount of examples from all other classes

"""

data.to_csv('unbalanced_data_psu.csv', index=False)

# find the minimum number of examples from all classes
min_class = min(
                len(data[data['sentiment'] == 'Positive']),
                len(data[data['sentiment'] == 'Neutral']),
                len(data[data['sentiment'] == 'Negative'])
               )
min_class

data.shape

# randomly sample data with the "min_class" amount 
sampled_positive = data[data['sentiment'] == 'Positive'].sample(n=min_class)
sampled_neutral = data[data['sentiment'] == 'Neutral'].sample(n=min_class)
sampled_negative = data[data['sentiment'] == 'Negative'].sample(n=min_class)

sampled_positive = sampled_positive.reset_index(drop=True)
sampled_neutral = sampled_neutral.reset_index(drop=True)
sampled_negative = sampled_negative.reset_index(drop=True)

data_dist = [
                len(sampled_positive),
                len(sampled_neutral),
                len(sampled_negative)
            ]
show_pie_chart(data_dist)

"""Finally, combine all classes together, shuffle them, and reset index for the whole data"""

balanced_data = sampled_positive.append(sampled_neutral).append(sampled_negative).sample(frac=1).reset_index(drop=True)
balanced_data

"""clean any emty row """

balanced_data = balanced_data[~balanced_data['normalized_text'].isna()].reset_index(drop=True)
len(balanced_data)

balanced_data.to_csv('balanced_data_psu.csv', index=False)

balanced_data.head

balanced_data.shape
